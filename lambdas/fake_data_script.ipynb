{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing relevant packages\n",
    "import boto3\n",
    "import pprint\n",
    "from boto3.dynamodb.conditions import Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up connection with dynamoDB\n",
    "region_name = 'us-east-1'\n",
    "dynamodb = boto3.resource('dynamodb', region_name=region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'app': 'asfpsfafas', 'num_devices': Decimal('600'), 'attributeTypes': ['N', 'N'], 'attributeRangeMaxes': ['1000', '12'], 'logo_image_url': 'bingus', 'attributeRangeMins': ['600', '4'], 'placeholder': 'placeholder', 'category': 'Consumer', 'dataset_id': 'henlo', 'attributes': ['wapapa', 'wowowow'], 'name': 'bingus'}\n"
     ]
    }
   ],
   "source": [
    "# function used to ensure I understood how to make calls to dynamo\n",
    "def get_table_stuff(table1):\n",
    "    \n",
    "    response = table1.get_item(Key={'dataset_id': 'henlo'})\n",
    "    \n",
    "    return response['Item']\n",
    "\n",
    "abc = get_table_stuff(table)\n",
    "print (abc)"
   ]
  },
  {
   "source": [
    "#### Simply building a fake dataset off of one example"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will only ever make a fake dataset from dataset table, so may as well declare it!\n",
    "table = dynamodb.Table('dataset_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"'0'\", \"'1'\"]\n"
     ]
    }
   ],
   "source": [
    "# more testing - this takes in a dataset id, converts it into a dictionary\n",
    "\n",
    "def preprocess_dynamo_row(column_header, input):\n",
    "\n",
    "    response = table.get_item(Key={column_header: input})\n",
    "    return response['Item']\n",
    "\n",
    "abc = (preprocess_dynamo_row('dataset_id', 'tobytest'))\n",
    "dict = abc['Correlation']\n",
    "print (list(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['wapapa', 'wowowow'], ['N', 'N'], 600, [[600.0, 1000.0], [4.0, 12.0]])\n"
     ]
    }
   ],
   "source": [
    "# this function will grab the relevant rows (column header, min, max, num_rows)\n",
    "def grab_relevant_info(column_header, input):\n",
    "\n",
    "    # this grabs the dictionary of the row of data we want!\n",
    "    dict = preprocess_dynamo_row(column_header, input)\n",
    "\n",
    "    columns = []\n",
    "    schema = []\n",
    "    num_rows = 0\n",
    "    ranges = []\n",
    "    #correlation = []\n",
    "\n",
    "    # grab the column headers; we assume that this is the right length of columns the data should be.\n",
    "    columns = dict['attributes']\n",
    "\n",
    "    # grab the data schema, we assume that this is per row (i.e. if 3 columns of data, this is the same length)\n",
    "    schema = dict['attributeTypes']\n",
    "\n",
    "    # grab the number of rows (fake data)\n",
    "    num_rows = int(dict['num_devices']) # this avoids any problem with that it is a Decimal value\n",
    "\n",
    "    \n",
    "    # to create ranges we need to make sure our data is not all string and/or binary\n",
    "    num = 0 # we assume it IS all string/binary, the schema must prove us wrong\n",
    "    for i in range(len(schema)):\n",
    "        if schema[i] == \"N\":\n",
    "            num = 1\n",
    "    \n",
    "    # this makes sure there is one number, i.e. there has to be a range!       \n",
    "    # if the bool is false there is no 'attributerange...' hence we need to instantiate a list somehow\n",
    "    if (num == 1):\n",
    "        mins = dict['attributeRangeMins']\n",
    "        maxs = dict['attributeRangeMaxes']\n",
    "    else:\n",
    "        mins = []\n",
    "        maxs = []\n",
    "    \n",
    "    # corresponding min and max of column; if string, make [0, 0]\n",
    "    min_max_counter = 0 # this tracks the smaller array\n",
    "\n",
    "    # go through each column and figure out if its range exists OR we must add a value\n",
    "    # suppose schema = [S, N, S], then range only equals [[3, 10]], must append [0, 0] for index 0 and 2 so we can iterate cleanly in other functions\n",
    "    for i in range(len(columns)):\n",
    "        if schema[i] == 'S':\n",
    "            ranges.append([])\n",
    "            ranges[i].append([0, 0])\n",
    "        elif schema[i] == 'B':\n",
    "            ranges.append([[]])\n",
    "            ranges[i].append([0, 1])\n",
    "        else:\n",
    "            ranges.append([])\n",
    "            ranges[i].append(float(mins[min_max_counter]))\n",
    "            ranges[i].append(float(maxs[min_max_counter]))\n",
    "            min_max_counter += 1\n",
    "    \n",
    "    # find correlation; this will be later. this supposes that we have another column of length n where 0 = no correlation with anyone else\n",
    "    # a non-zero means that this column is a function of another column (so each non-zero must occur at least twice!)\n",
    "    # a function of the other column goes both ways which enables flexibility in terms of how we write the code (refer to my last PR in correlated_fake_data in our experimental repo)\n",
    "    # correlation = list(dict['Correlation'])\n",
    "\n",
    "    return columns, schema, num_rows, ranges #,correlation\n",
    "\n",
    "test_data = grab_relevant_info('dataset_id', 'henlo')\n",
    "print (test_data)\n",
    "test_columns = test_data[0]\n",
    "test_schema = test_data[1]\n",
    "test_num_rows = test_data[2]\n",
    "test_ranges = test_data[3]\n",
    "# test_correlation = test_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[0, 0]], [[0, 0]], [3.0, 4.0], [[0, 1]]]\n"
     ]
    }
   ],
   "source": [
    "# test function to confirm the logic works; shows how we can append ranges\n",
    "schema = ['S', 'S', 'N', 'B']\n",
    "mins = [3]\n",
    "maxs = [4]\n",
    "def test(schema, mins, maxs):\n",
    "    ranges = []\n",
    "    min_max_counter = 0 # this tracks the smaller array\n",
    "    # hard to test this given the DynamoDB oddity of data, so this is just a manual test\n",
    "    for i in range(len(schema)):\n",
    "        if schema[i] == 'S':\n",
    "            ranges.append([])\n",
    "            ranges[i].append([0, 0])\n",
    "        elif schema[i] == 'B':\n",
    "            ranges.append([])\n",
    "            ranges[i].append([0, 1])\n",
    "        else:\n",
    "            ranges.append([])\n",
    "            ranges[i].append(float(mins[min_max_counter]))\n",
    "            ranges[i].append(float(maxs[min_max_counter]))\n",
    "            min_max_counter += 1\n",
    "            \n",
    "    return ranges\n",
    "print (test(schema, mins, maxs))"
   ]
  },
  {
   "source": [
    "#### The below function takes in column, range, and the number of rows and produces randomized integers in the range of each column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'of', 'and', 'to', 'a']\n"
     ]
    }
   ],
   "source": [
    "# this figures out the words.txt (taken from https://github.com/first20hours/google-10000-english/blob/master/20k.txt) \n",
    "# I took the 1k most popular words)\n",
    "f = open('words.txt', 'r')\n",
    "content = f.read()\n",
    "word_list = str.split(content)\n",
    "print (word_list[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['a', 'b', 'c', 'd'], ['search', 104890, 1, 6], ['again', 69506, 1, 7]]"
      ]
     },
     "metadata": {},
     "execution_count": 175
    }
   ],
   "source": [
    "# inputs: columns, discrete/continuous, ranges for each column, correlation, number of rows\n",
    "# this has every column besides the first column be a function of the first column\n",
    "# correlation: index of length n, 0 means no correlation with anything else, any non-zero int needs to have a pair (should be validated, will not be yet)\n",
    "# assume earlier index is the one the later one is based on... doesn't really matter\n",
    "def schematic_fake_data(columns, schema, ranges, num_rows):\n",
    "    \n",
    "    # input validation\n",
    "    if not (len(columns) == len(ranges)):\n",
    "        return (\"incorrect lengths of columns in one of the first three inputs\")\n",
    "    elif (num_rows <= 0):\n",
    "        return (\"incorrect number of rows\")\n",
    "\n",
    "    # # this is a dictionary on that tracks the counts of each value in array\n",
    "    # correlation_counter_dict = {}\n",
    "    # validation_counter = 0\n",
    "\n",
    "    # while validation_counter < len(correlation):\n",
    "    #     if correlation[validation_counter] in correlation_counter_dict.keys():\n",
    "    #         correlation_counter_dict[correlation[validation_counter]] += 1\n",
    "    #     else:\n",
    "    #         correlation_counter_dict[correlation[validation_counter]] = 1\n",
    "\n",
    "    #     validation_counter += 1\n",
    "    # val_list = (list(correlation_counter_dict.values()))\n",
    "    # key_list = (list(correlation_counter_dict.keys()))\n",
    "    # for i in val_list:\n",
    "    #     if i < 2:\n",
    "    #         if key_list[val_list.index(i)] != 0:\n",
    "    #             return (\"error: need a corresponding value with this correlation value\")\n",
    "    \n",
    "    output_data = [columns]\n",
    "\n",
    "    col_iterator = 0\n",
    "    row_of_data = 1 # topline is the headers\n",
    "    # write an array where each item is a row of fake data\n",
    "\n",
    "    while row_of_data <= num_rows:\n",
    "\n",
    "        output_data.append([])\n",
    "\n",
    "        while (len(columns) > col_iterator):\n",
    "\n",
    "            # # if this is a \"correlated column\", need to find other column and make it a function of another  \n",
    "            # if (correlation[col_iterator] != 0):\n",
    "                \n",
    "            #     # this finds if this current column is later\n",
    "            #     temp = 0\n",
    "            #     while temp < col_iterator:\n",
    "\n",
    "            #         # this means that this column is later\n",
    "            #         if correlation[temp] == correlation[col_iterator]:\n",
    "            #             output_data[row_of_data].append(output_data[row_of_data][temp] * 3 + random.random() * 5)\n",
    "            #             temp = col_iterator + 2 # to cut out of the loop\n",
    "            #             col_iterator += 1\n",
    "            #         else: \n",
    "            #             temp +=1\n",
    "                \n",
    "            #     # if this column index is earlier, make it random\n",
    "            #     if (schema[col_iterator] == \"int\"):\n",
    "            #         output_data[row_of_data].append(random.randint(range[col_iterator][0], range[col_iterator][1]))\n",
    "            #         col_iterator += 1\n",
    "            #     else:\n",
    "            #         output_data[row_of_data].append(range[col_iterator][0] + random.random() * (range[col_iterator][1] - range[col_iterator][0]))\n",
    "            #         col_iterator += 1\n",
    "\n",
    "            #else: \n",
    "            # if an integer or binary, which means they have an ACTUAL range, we select integer between the two!\n",
    "            if (schema[col_iterator] == \"N\" or schema[col_iterator] == \"B\"):\n",
    "                output_data[row_of_data].append(random.randint(ranges[col_iterator][0], ranges[col_iterator][1]))\n",
    "                col_iterator += 1\n",
    "\n",
    "            # if it a string, we just add a random word from our list of 1k words\n",
    "            elif (schema[col_iterator] == \"S\"):\n",
    "                rand_word = (word_list[random.randint(0, len(word_list) - 1)])\n",
    "                output_data[row_of_data].append(rand_word)\n",
    "                col_iterator += 1\n",
    "\n",
    "            # if not an int, a double, so we get a random double from min to max (i.e. min + (random decimal from 0 --> 1)(max-min))\n",
    "            else:\n",
    "                output_data[row_of_data].append(ranges[col_iterator][0] + random.random() * (ranges[col_iterator][1] - ranges[col_iterator][0]))\n",
    "                col_iterator += 1\n",
    "\n",
    "        col_iterator = 0\n",
    "        row_of_data += 1\n",
    "    \n",
    "    return output_data\n",
    "\n",
    "# test 1 - all kinds, make sure things are outputted correctly\n",
    "schematic_fake_data(['a', 'b', 'c', 'd'], ['S', 'N', 'B', 'N'], [[0, 0], [123, 123123], [0, 1], [5, 7]], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def convert_to_csv(columns, schema, ranges, num_rows, file_name):\n",
    "    output = schematic_fake_data(columns, schema, ranges, num_rows)\n",
    "\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        x = 0\n",
    "        while x < len(output):\n",
    "            spamwriter.writerow(output[x])\n",
    "            x += 1\n",
    "\n",
    "convert_to_csv(test_columns, test_schema, test_ranges, test_num_rows, 'fakedata/no_correlation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_to_csv(column_header, input):\n",
    "    # first, get the dictionary\n",
    "    right_info = grab_relevant_info(column_header, input)\n",
    "    # parse the right info into the four values we need\n",
    "    test_columns = right_info[0]\n",
    "    test_schema = right_info[1]\n",
    "    test_num_rows = right_info[2]\n",
    "    test_ranges = right_info[3]\n",
    "    \n",
    "    # write the csv\n",
    "    filename = ('fakedata/' + input + '.csv')\n",
    "    convert_to_csv(test_columns, test_schema, test_ranges, test_num_rows, filename)\n",
    "\n",
    "query_to_csv('dataset_id', 'henlo')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}